{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvunvfYJHNZN"
   },
   "source": [
    "# Transformer Day Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "859IyrS2HTKW",
    "outputId": "3453fd0f-82c8-4bb8-c54c-720f9012bcd5"
   },
   "outputs": [],
   "source": [
    "# Set Up\n",
    "#!git clone https://github.com/LxMLS/lxmls-toolkit.git\n",
    "#%cd lxmls-toolkit/\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../../../\")\n",
    "#sys.path.append(os.getcwd())\n",
    "#sys.path.append(\"/Users/israfelsalazar/Documents/lxmls-toolkit\")\n",
    "#!git checkout transformer-day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gU-ZcC4gH0ii"
   },
   "source": [
    "## Exercise 1: Tokenization\n",
    "\n",
    "*Tokenization* is a fundamental process in modern NLP pipelines. It works by splitting a sentence, which consists in a sequence of characters, into atomic elements called **tokens**, possibly discarding other elements like punctuations. These tokens are the basic language units used by the model. \n",
    "\n",
    "As we will see, the granularity of such units may cary. \n",
    "In general, *tokenization allows us to represent text data in a format that can be process by standar deep learning models*. In this exercise, we will explore tokenization to understand how it helps in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUW-9sBfIchS"
   },
   "source": [
    "### Word-based tokenizers\n",
    "\n",
    "The easiest way to split a text into units is probably to divide it based on whitespace. The idea is to chunk a sentence into segments everytime a space charecter is found. In python, we can do this with the `split()` function.\n",
    "\n",
    "_string.split(separator, maxsplit)_\n",
    "\n",
    "where separator is the whitespace by default and there's not maxsplit. You can read the python docs for `split()` [here](https://docs.python.org/3/library/stdtypes.html#str.split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQ_7lyF-IHgU"
   },
   "outputs": [],
   "source": [
    "text = \"I travelled to Lisbon in July to attend an NLP summer school\"\n",
    "text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzDfuMOo66dx"
   },
   "source": [
    "Now, this list of words can be fed to a model to perform task like next token prediction, machine translation, story generation etc.\n",
    "\n",
    "Tokenizing based on words allows the model to understand the basic units of language withouth worrying to learn things like workd boundaries. However, a downside of this approach is that we end up having an extremely large vocabulary, with an entry for each word of the language.\n",
    "Due to computational and processing resources, deep learning models still struggle to handle vocabularies that are larger than tens of thousands of tokens. For this reasons, some words need to be left out of the vocabulary and are all mapped to a shared token usually called UNK, which stands for unknown (word).\n",
    "\n",
    "#### Text normalization\n",
    "\n",
    "In actual tokenization pipelines, text is usually normalized before being tokenized. This process means extracting a basic version of each word that is stripped from suffixes or functional information. For example, the verb \"run\" might appear as \"running,\" \"runs\" or \"ran\", and the word \"program\" might appear as \"programmer\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-based tokenizers\n",
    "\n",
    "Another option is to tokenize text based on **characters**. This allows us to have a much smaller model vocabulary, it provides us with a method to handle new words given the combinatorial ability of combining known characters, but it doesn't instill to the model the concept of words, which should be learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I will travel to Lisbon in July, I will attend an NLP summer school but I hope to visit around: it's a beatiful city!\"\n",
    "tokenized = [c for c in text if c not in [\",\", \";\", \":\", \"'\", \"!\", \"?\"]]\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** What other problem are character-based tokenizers posing to NLP models?\n",
    "\n",
    "**Your Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best of both worlds: subword-based tokenizers\n",
    "\n",
    "In order to combine the best of both words, the most frequent tokenization strategy for modern NLP system is to tokenize based on **subwords**. Subwords are sequence of characters the can be shorter than entire words. \n",
    "\n",
    "How to split words into subwords depends on *how frequent a given sequence of characters is*. The core idea is that very frequent sequences are not split given that they are very likely to be used and appear a lot in corpora.\n",
    "\n",
    "For instance, \"unexpectedly\" can be a **rare word** in a corpus and being split in the subwords \"un\", \"expected\", \"ly\".\n",
    "These are standalone subwords that can be reused across other words, while the meaning of \"unexpectedly\" can be retained combining the three subwords. On the other hand, words like cats, people, and running, given their high frequency, will not be split by the tokenizer.\n",
    "\n",
    "\n",
    "#### BPE (byte-pair encoding) tokenizer\n",
    "\n",
    "One of the most widely used subword tokenizer is based on a method called **BPE (byte-pair encoding)**, which was introduced in a paper by [Senrich et al, 2016](https://aclanthology.org/P16-1162/). BPE relies on an *iterative algorithm based on the frequency of character sequences*. At each step, the algorithm compute character frequencies and merges pairs that tend to occur together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "\n",
    "Note that after we split words into subwords, we are now left with all the elements that will form the *vocabulary of our model*. Each subword is then mapped into an **index in the model vocabulary**. This is a standard mapping between string respresentations of (sub)words and vocabulary entries. \n",
    "\n",
    "Remember that subword tokenizers need to be \"trained\", they need to learn what word splits are based on data that they see. This is not the same training that we do with our neural network models given that tokenizers are *non-parametric deterministic methods*. \n",
    "\n",
    "Lastly, note also that different data leads to different word splitting choices and that a tokenizer is directly connected to a model. For this reason, you have keep in mind that a model trained on a given tokenizer it is not guaranteed to perform equally when coupled with a different tokenizer.\n",
    "\n",
    "##### Extra:\n",
    "\n",
    "Each vocabulary index, which corresponds to a subword, is then used by the model to load and process a related (sub)word embedding representation. These are dense vectors that the model will use when doing computation for any NLP task. If you want to learn more about word embeddings, check out this [website](https://lena-voita.github.io/nlp_course/word_embeddings.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BPE\n",
    "\n",
    "We are now looking at a real example using BPE. We can import the BPE tokenizer from the lxmls toolkit. BPE, which is used in models like GPT-2, and other commonly used subword tokenizers like WordPiece (used in BERT) are available in practically any standard NLP library like huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.transformers.bpe import BPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split the sentence: *\"Your drawing is charmingly anachronistic.\"*\n",
    "\n",
    "**Question:** Do you have a guess on which word will be split subwords and which one won't?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sample sentence\n",
    "sentence = \"Your drawing is charmingly anachronistic.\"\n",
    "tokenizer.encoder.encode_and_show_work(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at this python vocabulary object returned by the tokenizer. If you look at `bpe_idx`, you see all the subwords that the tokenizer decided to split. These number are the indexes in the vocabulary all our model. The `parts` field contains the actual splittting in the `token_merged` field. \n",
    "\n",
    "As you can see, some words have been split. Do they match your initial guess? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whitespaces\n",
    "\n",
    "You probably have noticed a special `Ġ` character inserted before each word expect the first one. This is because spaces are converted into this special token by the BPE algorithm, such that the word \"run\" and \" run\" are not treated equally and the tokenizer understands whether a word is at the beginning of a sentence or not. \n",
    "\n",
    "This is something that was found to provide better performance to the original GPT2 model. For more information you can read [here](https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475?u=joaogante)\n",
    "\n",
    "Now look at the next two tokenized sentence. Can you notice how the word \"very\" is assigned two different tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"running is very cool\"\n",
    "tokenizer.encoder.encode_and_show_work(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"very cool!\"\n",
    "tokenizer.encoder.encode_and_show_work(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling typos\n",
    "\n",
    "Now we are going to tokenize another two very similar sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZcpnDcwVb7Xw",
    "outputId": "0ee4d24a-d2fd-42ee-c396-ec29c7afd06f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = \"I like to circumnavigate the globe every year\"\n",
    "tokenizer.encoder.encode_and_show_work(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = \"I like to cirkumnavigate the globe every year\"\n",
    "tokenizer.encoder.encode_and_show_work(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why have they been tokenized differently? \n",
    "\n",
    "The only difference between these two sentences is in the the **typo** of the word \"circumnavigate\". As you can see, a _simple change_ in the word spelling breaks the tokenization process and leads to a different result. However, unlike word-based tokenizers where the wrong word would have been processed as an _unknown_ word, here we can still retain some ther other correct characters and our favorite NLP model can hopefully partially make it up for the typo while processing the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determinism\n",
    "\n",
    "Finally, recall that another important aspect of the tokenization process is that it's fully deterministic. Once we split a sentence into chunks and obtain the list of word indexes, we can fully revert the process and decode back the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \"We are about to start exercise 2 about attention, let's have fun!\"\n",
    "tokenized_sentence = tokenizer.encoder.encode(original_sentence)\n",
    "reconstructed_sentence = tokenizer.encoder.decode(tokenized_sentence)\n",
    "\n",
    "print(reconstructed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGe4ha9NgfGy"
   },
   "source": [
    "## Exercise 2: Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYj_37iZgjo9"
   },
   "source": [
    "Attention is a crucial component in the transformer, it allows to capture dependencies between different positions of two sequence of elements. In our case, and in most cases in NLP applications, sequences are sentences and elements are (sub)words.\n",
    "It is a powerful operation that allows to learn an alignment between each element in two sequences. It generates a score of how related each element in sequence1 and sequence2 are between each other.\n",
    "Understanding how attention works and being able to implement it are essential for anyone working with transformers. \n",
    "\n",
    "Given a query ($Q$), key ($K$), and value ($V$) tensors, the attention mechanism computes a weighted sum of the value tensor based on the similarity between the query and key tensors as shown in the following equation:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}\\Big(\\frac{QK^T}{\\sqrt{d_k}}\\Big)V\n",
    "$$\n",
    "\n",
    "where \n",
    "- $Q$ represents the query tensor.\n",
    "- $K$ represents the key tensor.\n",
    "- $V$ represents the value tensor.\n",
    "- $d_k$ represents the dimensionality of the key tensor.\n",
    "\n",
    "This is the image that was in the original Transformer paper and that shows the computations used in the attention.\n",
    "\n",
    "Forget about the right part, we'll get back to that later in the lab.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1270/1*LpDpZojgoKTPBBt8wdC4nQ.png)\n",
    "\n",
    "\n",
    "In this exercise, we will dive into the attention mechanism. To do so, we are going to build a simple cross-attention function that we will then extend to a more complex multi-head self-attention module that incorporates the concept of causality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0lBpBvtiEQX"
   },
   "source": [
    "### Exercise 2.1: Building a Simple Cross-Attention Function\n",
    "\n",
    "Cross-attention refers to the case where the input sequences to compute $Q$, $K$, and $V$ come from different sources. It allows models to incorporate contextual information from one sequence (S1) into another (S2). <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1)\n",
    "\n",
    "\n",
    "Given two input sequences $S_1$ and $S_2$ and the transformation weights $W_Q$, $W_K$ and $W_V$, complete the `cross_attention` function in the cell below. \n",
    "\n",
    "You need to implement the following:\n",
    "- Calculate the query, key, and value projections using linear transformations.\n",
    "- Compute the attention scores by performing the dot product between the query and key tensors.\n",
    "- Apply softmax activation to the attention scores to obtain the attention weights.\n",
    "- Multiply the attention weights with the value tensor to get the attended values.\n",
    "- Return the attended values.\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Conceptually, the self attention variant that you might have heard is the same, with the only difference that the S1 and S2 sequences are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Matrix sizes\n",
    "\n",
    "- q: query size\n",
    "- d: hidden dimension\n",
    "- c: context length\n",
    "---\n",
    "- Q: 1xqxd\n",
    "- K, V: 1xcxd\n",
    "- Q x K:  1xqxd x 1xsxd.T \n",
    "- (QK) x V:  1xqxs  x  V  1xsxd\n",
    "---\n",
    "- Attn: 1xqxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFYCyf-GgKly"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cross_attention(S1, S2, W_Q, W_K, W_V):\n",
    "    # Calculate Queries from sequence S2\n",
    "    queries = torch.matmul(S2, W_Q)\n",
    "\n",
    "    # Calculate Key and Value from sequence S1\n",
    "    keys = torch.matmul(S1, W_K)\n",
    "    values = torch.matmul(S1, W_V)\n",
    "    \n",
    "    # Computing attention\n",
    "    attention_scores = torch.matmul(queries, keys.transpose(-2,-1))\n",
    "    \n",
    "    # Scale the attention scores\n",
    "    d_k = queries.size(-1)\n",
    "    attention_scores = attention_scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    # Apply softmax to obtain attention weights\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1) # 1, 4, 13\n",
    "    \n",
    "    # Compute the attended values\n",
    "    attended_values = torch.matmul(attention_weights, values) # 1, 13, 2\n",
    "    \n",
    "    return attended_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context\n",
    "S1 = torch.rand((1,13,3))\n",
    "\n",
    "# Query\n",
    "S2 = torch.rand((1,4,3))\n",
    "\n",
    "# Projections\n",
    "W_Q = torch.rand((3, 2))  # Query weights\n",
    "W_K = torch.rand((3, 2))  # Key weights\n",
    "W_V = torch.rand((3, 2))  # Value weights\n",
    "\n",
    "# Perform cross-attention\n",
    "attended_values = cross_attention(S1, S2, W_Q, W_K, W_V)\n",
    "\n",
    "# Expected output # 1,4,2 (B, Sequence, Projection)\n",
    "print(f\"Output Shape: {attended_values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gs7ar9XJu0u6"
   },
   "source": [
    "### Exercise 2.2: Extending to Multi-Head Self-Attention\n",
    "\n",
    "#### Update the class CausalSelfAttention on lxmls/transformers/model.py\n",
    "\n",
    "Great! You have successfully implemented cross-attention. Now, let's make some modifications so we can train a real GPT model.\n",
    "\n",
    "**1. Self-Attention**\n",
    "\n",
    "We will be replacing the cross-attention mechanism with self-attention. In self-attention, a single sequence acts as the query, key, and value, allowing attention to be computed within the sequence itself. This can be useful for syntactic where an attention head can model the relationship between part of speech like subjects and verbs. \n",
    "\n",
    "**2. Multi-Head**\n",
    "\n",
    "However, the relations present even in a single sentence are more than one. Think about number and gender agreement as one, the semantic relation between subject and object, the functional aspect that verb arguments have etc. All this cannot be modeled by a single head.\n",
    "\n",
    "For this reason, we are going to extend the single-head attention function to **multi-head attention**. In the previous implementation, we had one set of weights for the input query, resulting in a single type of _relationship between the the source and target sequence_. With multi-head attention, we can utilize _multiple parallel single-head attention modules_ to obtain diverse relationships between the query and the values. The attention operation works by projecting the sequences through a multiplication with a projection matrix, and then computing the alignment score. These are are all operation that can be parallelized since there's no interdependency between each each head. For this reasons, each head could learn to model a different linguistic intereation useful for many downstream tasks, be it syntactic, semantic or generation-based..\n",
    "\n",
    "**3. Pytorch Module**\n",
    "\n",
    "The last modification involves embedding our function into a PyTorch module. As you may have noticed, in the previous exercise, we passed the transformation weights as inputs to the function. In a real-world scenario, these matrices are learned, and PyTorch can keep track of them for us.\n",
    "\n",
    "Complete the missing lines on the initialization of the module and the forward pass.\n",
    "\n",
    "###### Note\n",
    "\n",
    "GPT uses a version of self-attention called causal self-attention. When training our models for tasks like language modeling and machine translation, in practice we feed the entire train sequence to the model but, at every timestep, we want to prevent it to compute the alignment with future tokens. For this reason we use a mask that we incrementally lift at every timestep. For instance, we have a sentence that says \"Libson is a great city to live in\". At time 0, we feed the entire sentence to the model masking everything but the first token. Using the strikethrough format as masking, this will be what the model sees at step 0:\n",
    "\n",
    "- Time 0: Libson ~is a great city to live in~\n",
    "\n",
    "We then let the model generate a token a and move to step 1 where we are masking everything but the first two tokens\n",
    " \n",
    "- Time 1: Libson is ~a great city to live in~ \n",
    "\n",
    "and so on...\n",
    "\n",
    "- Time 2: Libson is a ~great city to live in~ \n",
    "- Time 3: Libson is a great ~city to live in~ \n",
    "- Time 4: Libson is a great city ~to live in~ \n",
    "- Time 5: Libson is a great city to ~live in~ \n",
    "- Time 6: Libson is a great city to live ~in~ \n",
    "\n",
    "We can now look back at the attention figure from the paper. Hopefully, you are now able to understand also the right side of the figure.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1270/1*LpDpZojgoKTPBBt8wdC4nQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHx7_naBIVU9"
   },
   "source": [
    "### Exercise 2.3: Questions [Optional]\n",
    "<details>\n",
    "<summary>What is the purpose of applying a causal mask in the attention computation?</summary>\n",
    "The causal mask ensure that the in attention computation, each position in the sequence can only attend to the positions on its left, preventing information leakage from future positions. This is essential in tasks where the model should generate output sequentially, such as language generation or autoregressive tasks.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>How does the number of attention heads affect the model's capacity to capture different types of dependencies in the input sequence?</summary>\n",
    "Multiple heads allow the model to attend to different parts of the input sequence simultaneously. By increasing the number of attention heads, the model can capture more diverse dependencies and patterns in the data. Each head can focus on different aspects of the input, enabling the model to learn complex relationships and improve performance on tasks that require capturing multiple types of dependencies.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>What is the purpose of the residual dropout and attention dropout in the CausalSelfAttention module?</summary>\n",
    "The residual dropout and attention dropout are regularization techniques used to prevent overfitting and improve the generalization of the model. The residual dropout applies dropout to the output of the attention module, helping to regularize the model during training. The attention dropout applies dropout to the attention weights, which helps to reduce over-reliance on specific tokens and encourages the model to attend to a broader range of tokens in the sequence.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: Visualize Attentions[Optional]\n",
    "\n",
    "Now that we understand the basic mechanisms of attention, we can check the activated attention patterns in a pretrained BERT model (Devlin et al. 2018). Recall that BERT is an encoder-based transformer model which is based on a stack of self-attention blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uI-bIuxUteuC",
    "outputId": "40ee1d8b-a37f-4c58-dbcb-6835d83c298a"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from bertviz import head_view\n",
    "\n",
    "# Define a sample input text\n",
    "text = \"I will go for a run and will jump into a lake.\"\n",
    "\n",
    "# Instantiate the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Convert tokens to token IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = [1] * len(token_ids)\n",
    "\n",
    "# Convert token IDs and attention mask to tensors\n",
    "input_ids = torch.tensor([token_ids])\n",
    "attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "# Generate the transformer output\n",
    "outputs = model(input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "\n",
    "# Extract attentions and check the shape\n",
    "outputs.attentions[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we extracted an attention from the first layer. The first dimension is the bach, the second one is the number of heads used in the first layer, and the last two dimensions are the sequence length. Given that this was a self attention block the last two numbers are equal.\n",
    "\n",
    "We can now use a method from the [bertviz library](https://github.com/jessevig/bertviz) and plot all the heads.\n",
    "\n",
    "You'll see a dropdown menu that allows you the select a layer of the model (GPT-2 has 12). You'll then see a color for every head used in that layer (GPT-2 has 12 head per layer). By default all heads are shown, click on a color to activate/disactivate that head. It can help starting by activating only one head and checking the learned relation learn by that self attentino head. By hovering over each word you can see the attention weigths that linked that words to all the others.\n",
    "\n",
    "**Question** Do you notice any interesting (linguistic) pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(outputs.attentions, tokens=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRiFro12JhO3"
   },
   "source": [
    "## Exercise 3:\n",
    "Now that we know everything aboout attention, we can go ahead and train a GPT-based model which heavilty relies on attention.\n",
    "\n",
    "We will now:\n",
    "1. Create a GPT-2 model\n",
    "2. Train this model on a small dataset.\n",
    "3. Check the loss of our model \n",
    "\n",
    "Based on this\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Training a Weather Prediction Model using Autoregressive Transformer\n",
    "\n",
    "In this exercise, we will work with a dummy weather dataset that consists of sequences of weather observations and corresponding states. The goal is to train a small model using an autoregressive transformer to predict the weather state based on the previous observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZtJ6vYpmGye"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from lxmls.transformers.utils import set_seed\n",
    "from lxmls.transformers.bpe import BPETokenizer\n",
    "from lxmls.transformers.model import GPT\n",
    "from lxmls.transformers.trainer import Trainer\n",
    "from lxmls.transformers.dataset import WeatherDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by initializing the dataset, which is responsible for providing the training data for our model. The dataset contains sequences of weather observations and their corresponding states. These sequences are converted into indices and concatenated to form the input and output sequences for the transformer model.\n",
    "\n",
    "You can check in detail the dataset in `lxmls/transformers/dataset.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiQSE-kDs9Eg"
   },
   "outputs": [],
   "source": [
    "# Fixed probabilities, easier to learn\n",
    "# This is just to create the sequence in the dataset\n",
    "fixed_proba = {}\n",
    "fixed_proba[\"initial\"] = [.5,.3,.2]\n",
    "fixed_proba[\"transition\"] = [\n",
    "    [.5,.5,0],\n",
    "    [0,.5,.5],\n",
    "    [.5,0,.5]\n",
    "]\n",
    "fixed_proba[\"emission\"] = [\n",
    "    [.5,0,.2,0,.3],\n",
    "    [0,.5,.4,0,.1],\n",
    "    [0,0,.1,.5,.4]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ytVEbaUOsvrQ",
    "outputId": "203d347f-c907-41b1-f56e-86b7ce6662b6"
   },
   "outputs": [],
   "source": [
    "# print an example instance of the dataset\n",
    "train_dataset = WeatherDataset('train', proba=fixed_proba)\n",
    "test_dataset = WeatherDataset('test', proba=train_dataset.proba)\n",
    "x, y = train_dataset[0]\n",
    "\n",
    "print(\"Sampling from the dataset:\")\n",
    "print(f\"Input: {train_dataset.decode_obs(x.tolist()[:6])}\")\n",
    "print(f\"Labels: {train_dataset.decode_st(y.tolist()[5:])}\")\n",
    "print(\"-\"*50)\n",
    "print(\"Tokenized sequences:\")\n",
    "print(f\"Input: {x.tolist()}\")\n",
    "print(f\"Labels: {y.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a model using the default configuration for the GPT model. This configuration includes parameters which determine the size and structure of the model. The GPT model is a small version called GPT Nano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVP-xVcvsmxa",
    "outputId": "91d7092b-fa70-4f0a-9582-89ef49a88712"
   },
   "outputs": [],
   "source": [
    "# create a GPT instance\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model = GPT(model_config)\n",
    "\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we create a Trainer object. The Trainer handles the training process, including defining the learning rate, setting the maximum number of iterations, and specifying the number of workers for data loading. The Trainer is initialized with the model, training dataset, and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1rWWFRPFsqCj",
    "outputId": "18f15270-a5d0-44f7-c023-57135f061946"
   },
   "outputs": [],
   "source": [
    "# create a Trainer object\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 2000\n",
    "train_config.num_workers = 0\n",
    "train_config.device = \"mps\"\n",
    "trainer = Trainer(train_config, model, train_dataset)\n",
    "\n",
    "print(train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these components in place, we are ready to train our model on the weather dataset and make predictions based on the learned patterns. We just add some minor utilities function that show us intermediate logs. You can safely ignore them since most of this is usually abstracted away from end users in modern deep learning libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZg0Z-GFtDMN",
    "outputId": "5d5771e8-8495-4f06-c738-42e23043572b"
   },
   "outputs": [],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.run()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print the training time\n",
    "print(\"Training time: {:.2f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the loss started decreaseing and it seemed to fluctuate around a range of values close to 0.25\n",
    "\n",
    "Great! You have just **trained a small GPT model**! Congrats!\n",
    "Generating from such a tiny model that has been trained only for a short number of iteration won't give us interesting output. Let's rely on the one of the many powerful and larger pretrained model publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Prompting a pretrained GPT-2 model\n",
    "\n",
    "We can load a pretrained gpt2 model from hugging face (this is done behind the scene from the GPT class) and prompt it with any text of our choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'gpt2'\n",
    "device = 'mps' # <- this works for modern Mac devices, feel free to change it to 'cpu' in case you a different machine\n",
    "\n",
    "model = GPT.from_pretrained(model_type)\n",
    "\n",
    "# We move the model to device in case we want to exploit gpu acceleration\n",
    "# we also set it to eval mode since we are not interested in computing or storing any gradients\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can now generate from our pretrained model. We just need to pass a context and let the model generate. \n",
    "There are two additional parameters. `tokens` is the number of tokens we want our model to generate and `num_samples` is the number of diverse samples we are asking the model to produce. Since we are sampling from the model distribution, we can generate as many samples as we want, hence the parameter.\n",
    "\n",
    "Feel free to change the context and have fun _generating text from a pretrained **GPT2**_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random prompt, uses pooling\n",
    "for i in range(5): \n",
    "    set_seed(42)\n",
    "    model.prompt(\"Barack Obama, the\", 50, 3)\n",
    "\n",
    "# Deterministic prompt, does NOT use pooling\n",
    "for i in range(5):\n",
    "    model.prompt_topK(\"Barack Obama, the\", 50, 3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GAQAv4iil7LX"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "004553b714df4f9fa94e3e8aad429cb4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "00d8c994da6041949554985cc8425e8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "06bae1d9afcb42f79e8c9447f34568df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b49db4be6db4ca0aca576d90796e498": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10dd6e3f02dd4661abfaaf75b43e2914": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c46a02021334c9dafa7e3cbea7c3fd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ec5a1f535934e2ba6c4f67357ab97b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "208fa2e9acc8443698cf6f1cd0f7bf82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2584967c2b7b4821a8aa1ae1eaa602b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdfb7a69f3524fd3a66de434c0e03ecc",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_63e549e71a2b425dba863727737f53bb",
      "value": 570
     }
    },
    "30aba7c8bea241efb630266330eaec29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ec5a1f535934e2ba6c4f67357ab97b4",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_50f169d401e348b486e15a68b9f0bebf",
      "value": 231508
     }
    },
    "3c28f8163719481ba3d4cc34cb055060": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56e04d73c266491ebe0623f69f9ce763",
      "placeholder": "​",
      "style": "IPY_MODEL_208fa2e9acc8443698cf6f1cd0f7bf82",
      "value": " 440M/440M [00:04&lt;00:00, 103MB/s]"
     }
    },
    "411c6dab66b54654a0405f4df08c5639": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de646d633a914d9592090f65051a62ff",
      "placeholder": "​",
      "style": "IPY_MODEL_00d8c994da6041949554985cc8425e8e",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "481efe6d7c3a46d08a37619d8cd5809b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "484405deb40f462f81e82e6139d77fd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd02d3c7f1b54926be6f7599455fb024",
      "placeholder": "​",
      "style": "IPY_MODEL_0b49db4be6db4ca0aca576d90796e498",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "50f169d401e348b486e15a68b9f0bebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "56e04d73c266491ebe0623f69f9ce763": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63e549e71a2b425dba863727737f53bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "652d57b1fcc6438c993c8ef6fc0a14fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70381041f4544b04bb3bb9e49c8defc5",
      "placeholder": "​",
      "style": "IPY_MODEL_10dd6e3f02dd4661abfaaf75b43e2914",
      "value": " 28.0/28.0 [00:00&lt;00:00, 276B/s]"
     }
    },
    "67aef5b516a54f73bd18947635221ce3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_686bbfa0b07e449f84da2d23328aed52",
      "placeholder": "​",
      "style": "IPY_MODEL_8fa6940c2a1047f2a364bff4abc0776e",
      "value": " 570/570 [00:00&lt;00:00, 8.40kB/s]"
     }
    },
    "686bbfa0b07e449f84da2d23328aed52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69384a586dfb4bdcbe22ab9d66443ab5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70381041f4544b04bb3bb9e49c8defc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77e4148a5eff405a93f81cdf131daf3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a198c7be8974598a61e2f231502e2f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b983da7cd8f24ee4a3e2ee378c8e8d3b",
       "IPY_MODEL_cd2c2b9d0e2a4144970196a470ff6437",
       "IPY_MODEL_3c28f8163719481ba3d4cc34cb055060"
      ],
      "layout": "IPY_MODEL_004553b714df4f9fa94e3e8aad429cb4"
     }
    },
    "8fa6940c2a1047f2a364bff4abc0776e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac296dbdef4d40d38e7fdec2ebcd5382": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8bf52de9eae4860bd5c8a3c6d6e4bb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b983da7cd8f24ee4a3e2ee378c8e8d3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77e4148a5eff405a93f81cdf131daf3b",
      "placeholder": "​",
      "style": "IPY_MODEL_69384a586dfb4bdcbe22ab9d66443ab5",
      "value": "Downloading model.safetensors: 100%"
     }
    },
    "cd2c2b9d0e2a4144970196a470ff6437": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_481efe6d7c3a46d08a37619d8cd5809b",
      "max": 440449768,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e1130befba034636a0173fec87a18a92",
      "value": 440449768
     }
    },
    "cdfb7a69f3524fd3a66de434c0e03ecc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0a6f5622ecb4ef4a38c1ce90f15c072": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d6381879001341a18187c6e63f10c464": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d962809a60f64f8b9097a98fce3ea48d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dd02d3c7f1b54926be6f7599455fb024": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de646d633a914d9592090f65051a62ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1130befba034636a0173fec87a18a92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e5f17745469147cd9233eeb8b5b8461d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_484405deb40f462f81e82e6139d77fd6",
       "IPY_MODEL_fad47dcd32844881bb2b07606c54cd6d",
       "IPY_MODEL_652d57b1fcc6438c993c8ef6fc0a14fd"
      ],
      "layout": "IPY_MODEL_06bae1d9afcb42f79e8c9447f34568df"
     }
    },
    "e6e49d02bd8042648d91a81c1e7b06f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e728ccc9ef7a4ad78d2d64308495c322": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6e49d02bd8042648d91a81c1e7b06f2",
      "placeholder": "​",
      "style": "IPY_MODEL_d6381879001341a18187c6e63f10c464",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "eb7badb8586b466c9ea01d163c45e820": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eba9114e27f54e7f9f5926971e83de2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e728ccc9ef7a4ad78d2d64308495c322",
       "IPY_MODEL_30aba7c8bea241efb630266330eaec29",
       "IPY_MODEL_ecf82a9385c74c57811a5390693b475c"
      ],
      "layout": "IPY_MODEL_eb7badb8586b466c9ea01d163c45e820"
     }
    },
    "ecf82a9385c74c57811a5390693b475c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac296dbdef4d40d38e7fdec2ebcd5382",
      "placeholder": "​",
      "style": "IPY_MODEL_d962809a60f64f8b9097a98fce3ea48d",
      "value": " 232k/232k [00:00&lt;00:00, 1.89MB/s]"
     }
    },
    "f3e49e667464406cbd7a283c5dc6c354": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_411c6dab66b54654a0405f4df08c5639",
       "IPY_MODEL_2584967c2b7b4821a8aa1ae1eaa602b6",
       "IPY_MODEL_67aef5b516a54f73bd18947635221ce3"
      ],
      "layout": "IPY_MODEL_b8bf52de9eae4860bd5c8a3c6d6e4bb6"
     }
    },
    "fad47dcd32844881bb2b07606c54cd6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c46a02021334c9dafa7e3cbea7c3fd9",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d0a6f5622ecb4ef4a38c1ce90f15c072",
      "value": 28
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
