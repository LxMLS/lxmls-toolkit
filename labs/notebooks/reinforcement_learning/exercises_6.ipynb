{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Value Function\n",
    "\n",
    "Next we will find different ways to compute the Value function given by a stochastic policy $\\pi(s) = p(a\\mid s)$.\n",
    "We want to calculate $V_{\\pi}(s)$ (the state-value-function given a policy)\n",
    "Here, we draw an Markov Decision Process (MDP) with three states $\\mathcal{S}=\\{s_1,s_2,s_3\\}$ and three possible actions $\\mathcal{A}=\\{a_1,a_2,a_3\\}$, moving to state $s_1$, moving to state $s_2$ and moving to state $s_3$.\n",
    "![mdp.png](mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Policy Evaluation by Dynamic Programming\n",
    "\n",
    "For the MDP represented above we define the state transition probability matrix $\\mathcal{P}^a_{ss'}=p(S_{t+1}=s'\\mid S_{t}=s, A_t=a)$. In this MDP we assume that when we choose to move to state $s_i$, $i=\\{1,2,3\\}$ we always end up in that state, meaning that $\\mathcal{P}^a_{ss'}=p(S_{t+1}=s'\\mid S_{t}=s, A_t=a)=1$. In this case, $\\mathcal{P}^{\\pi}=\\mathcal{P}^a_{ss'}\\pi(a\\mid s) = \\pi(a\\mid s)$ the Bellman Expectation equation becomes (Check page 14 and 16 from the lecture slides.):\n",
    "$$\n",
    "V_{\\pi}(s) = \\sum_{a\\in\\mathcal{A}} \\pi(a\\mid s)\\left( \\mathcal{R}^a_s + \\gamma \\sum_{s'\\in \\mathcal{S}}\\mathcal{P}^a_{ss'}V_{\\pi}(s')\\right) = \\mathcal{R}^{\\pi}+ \\gamma \\sum_{s'\\in \\mathcal{S}}\\pi(a\\mid s)V_{\\pi}(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "policy=np.array([[0.3, 0.2, 0.5], [0.5, 0.4, 0.1], [0.8, 0.1, 0.1]])\n",
    "# 'raw_rewards' variable contains rewards obtained after transition to each state\n",
    "# In our example it doesn't depend on source state\n",
    "raw_rewards = np.array([1.5, -1.833333333, 19.833333333])\n",
    "# 'rewards' variable contains expected values of the next reward for each state\n",
    "rewards = np.matmul(policy, raw_rewards)\n",
    "assert np.allclose(rewards, np.array([10., 2., 3.]))\n",
    "\n",
    "state_value_function=np.array([0 for i in range(3)])\n",
    "\n",
    "for i in range(20):\n",
    "    print(state_value_function)\n",
    "    \n",
    "    state_value_function=#TODO: Implement the Policy Evaluation Update with a Discount Rate of 0.1\n",
    "print(state_value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6.1 Policy Evaluation by Linear Programming\n",
    "The state-value-function can be directly solved through linear programming (as shown on page 15 from the lecture slides):\n",
    "$$\n",
    "V_{\\pi}(s)=\\left(I-\\gamma\\mathcal{P}^{\\pi}\\right)^{-1}\\mathcal{R}^{\\pi}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution=#TODO: Implement the linear programming solution with a discount rate of 0.1\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should stay the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Monte Carlo Policy Evaluation\n",
    "\n",
    "We can design yet another way of evaluating the value of a given policy $\\pi$, see lecture slides pag.20.\n",
    "The intuition is to incrementally the expected return from sampled episodes, sequences of triplets $\\{(s_i,a_i,r_{i})\\}_{i=1}^N$. The function $\\color{blue}{gt}$ computes the total discounted reward from a list of sequential rewards obtained by sampling the policy over N time-steps: $G_t=r_t+\\gamma r_{t+1}+\\gamma^2 r_{t+2}+\\dots+\\gamma^N r_{t+N}$.\n",
    "\n",
    "The value of a policy can also be computed by looking at its empirical expected total discounted reward:\n",
    "$$\n",
    "V_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[G_t\\mid S_t=s\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "reward_counter=np.array([0., 0., 0.])\n",
    "visit_counter=np.array([0., 0., 0.])\n",
    "\n",
    "def gt(rewardlist, gamma=0.1):\n",
    "    '''\n",
    "    Function to calculate the total discounted reward\n",
    "    >>> gt([10, 2, 3], gamma=0.1)\n",
    "    10.23\n",
    "    '''\n",
    "    #TODO: Implement the total discounted reward\n",
    "    return 0\n",
    "\n",
    "\n",
    "for i in range(400):\n",
    "    start_state=random.randint(0, 2)\n",
    "    next_state=start_state\n",
    "    rewardlist=[]\n",
    "    occurence=defaultdict(list) \n",
    "    for i in range(250):\n",
    "        rewardlist.append(rewards[next_state]) \n",
    "        occurence[next_state].append(len(rewardlist)-1) \n",
    "        action=np.random.choice(np.arange(0, 3), p=policy[next_state]) \n",
    "        next_state=action\n",
    "\n",
    "    for state in occurence: \n",
    "        for value in occurence[state]: \n",
    "            rew=gt(rewardlist[value:]) \n",
    "            reward_counter[state]+=rew \n",
    "            visit_counter[state]+=1 \n",
    "            #break #if break: return following only the first visit\n",
    "\n",
    "print(reward_counter/visit_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As can be seen the result is nearly the same as the state-value-function calculated above.\n",
    "\n",
    "So far we have seen different ways of given a known policy $\\pi(a\\mid s)$ how to comput its value $V_{\\pi}(s)$. Next, we wish to find the optimal policy $\\pi^\\ast(s)$ for the MDP in the example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Policy Optimization by Q-Learning\n",
    "\n",
    "This code solves a very easy problem: using the rewards it calculates the optimal action-value-function (page 26 on slides).\n",
    "\n",
    "It samples a state-action pair randomly, so that all state-action pairs can be seen, and updates the matrix of Q-values (expected discounted reward under the policy), for each state-action pair $\\mathbf{Q[s_t, a_t] = (1-\\alpha)\\ Q[s_t, a_t] + \\alpha \\left( r_t + \\gamma \\max\\limits_{a} Q[s_{t+1}, a]\\right)}$ since we are in the tabular case--- where states and action are finite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table=np.zeros((3, 3)) \n",
    "for i in range(1001): \n",
    "    state=random.randint(0, 2) \n",
    "    action=random.randint(0, 2) \n",
    "    next_state=action\n",
    "    reward=rewards[next_state] \n",
    "    next_q=max(q_table[next_state]) \n",
    "    q_table[state, action]= #TODO: Implement the Q-Table update\n",
    "    if i%100==0:\n",
    "        print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extra: Value Iteration\n",
    "\n",
    "Next we compute the optimal policy by first optimizing the value function, via a fixed point method, using the Bellman optimality equation for the state value function: $ V(s) = \\max_{a\\in\\mathcal{A}}r_t + \\gamma \\sum_{s'\\in \\mathcal{S}}\\mathcal{P}^a_{s_t,s'}V(s')$, where for the given MDP $\\mathcal{P}^a_{ss'}=p(S_{t+1}=s'\\mid S_{t}=s, A_t=a)=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rewards=np.array([10., 2., 3.])\n",
    "gamma = 0.1\n",
    "\n",
    "state_value_function = np.zeros(3)\n",
    "\n",
    "for i in range(1000):\n",
    "    for s in range(3):\n",
    "        state_value_function[s]=#TODO: Implement the state value function update\n",
    "print(state_value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.4: Score Function Gradient Estimator\n",
    "\n",
    "Implement the score function gradient estimator of the REINFORCE algorithm in lxmls/reinforcement_learning/score\\_function\\_estimator.py. Check it is correct by calling the train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from lxmls.reinforcement_learning.score_function_estimator import train\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.5: Policy Gradient for the CartPole task\n",
    "\n",
    "Implement policy gradient for the cartpole task by coding the forward pass of Model() in lxmls/reinforcement\\_learning/policy\\_gradient.py. Check that it is correct by calling the train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.reinforcement_learning.policy_gradient import train\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: Actor Critic for the CartPole task\n",
    "Implement actor crtitic for the cartpole task by coding the critic forward pass in lxmls/reinforcement\\_learning/policy\\_gradient.py. Check it is correct by calling the train() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.reinforcement_learning.actor_critic import train\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6.6: Policy RNN for Part-of-Speech Tagging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As a last exercise, apply what you have learned to the RNN model seen in previous days. Implement REINFORCE to replace the maximum likelihood loss used on the RNN day. For this you can modify the PolicyRNN class in lxmls/deep learning/pytorch\\_models/rnn.py\n",
    "You can test your implementation by running the file lxmls/reinforcement_learning/RNN_Reinforce.py."
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
