{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Sentiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lxmls.readers.sentiment_reader as srs\n",
    "from lxmls.deep_learning.utils import AmazonData\n",
    "corpus = srs.SentimentCorpus(\"books\")\n",
    "data = AmazonData(corpus=corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2 Implement Backpropagation for an MLP in Numpy and train it\n",
    "Instantiate the feed-forward model class and optimization parameters. This models follows the architecture described in Algorithm 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "geometry = [corpus.nr_features, 20, 2]\n",
    "activation_functions = ['sigmoid', 'softmax']\n",
    "\n",
    "# Optimization\n",
    "learning_rate = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.numpy_models.mlp import NumpyMLP\n",
    "model = NumpyMLP(\n",
    "    geometry=geometry,\n",
    "    activation_functions=activation_functions,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestone 1:\n",
    "Open the code for this model. This is located in \n",
    "\n",
    "    lxmls/deep_learning/numpy_models/mlp.py\n",
    "    \n",
    "Implement the method `backpropagation()` in the class `NumpyMLP` using Backpropagation recursion that we just saw.\n",
    "\n",
    "As a first step focus on getting the gradients of each layer, one at a time. Use the code below to plot the loss values for the study weight and perturbed versions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.mlp import get_mlp_parameter_handlers, get_mlp_loss_range\n",
    "\n",
    "# Get functions to get and set values of a particular weight of the model\n",
    "get_parameter, set_parameter = get_mlp_parameter_handlers(\n",
    "    layer_index=1,\n",
    "    is_bias=False,\n",
    "    row=0, \n",
    "    column=0\n",
    ")\n",
    "\n",
    "# Get batch of data\n",
    "batch = data.batches('train', batch_size=batch_size)[0]\n",
    "\n",
    "# Get loss and weight value\n",
    "current_loss = model.cross_entropy_loss(batch['input'], batch['output'])\n",
    "current_weight = get_parameter(model.parameters)\n",
    "\n",
    "# Get range of values of the weight and loss around current parameters values\n",
    "weight_range, loss_range = get_mlp_loss_range(model, get_parameter, set_parameter, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have implemented at least the gradient of the last layer. You can start checking if the values match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the gradient value for that weight\n",
    "gradients = model.backpropagation(batch['input'], batch['output'])\n",
    "current_gradient = get_parameter(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can plot the values of the loss around a given parameters value versus the gradient. If you have implemented this correctly the gradient should be tangent to the loss at the current weight value, see Figure 3.5. Once you have completed the exercise, you should be able to plot also the gradients of the other layers. Take into account that the gradients for the first layer will only be non zero for the indices of words present in the batch. You can locate this using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this to know the non-zero values of the input (that have non-zero gradient)\n",
    "batch['input'][0].nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the following code for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot empirical\n",
    "plt.plot(weight_range, loss_range)\n",
    "plt.plot(current_weight, current_loss, 'xr')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('weight value')\n",
    "# Plot real\n",
    "h = plt.plot(\n",
    "    weight_range,\n",
    "    current_gradient*(weight_range - current_weight) + current_loss, \n",
    "    'r--'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestone 2:\n",
    "After you have ensured that your Backpropagation algorithm is correct, you can train a model with the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get batch iterators for train and test\n",
    "train_batches = data.batches('train', batch_size=batch_size)\n",
    "test_set = data.batches('test', batch_size=None)[0]\n",
    "\n",
    "# Epoch loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Batch loop\n",
    "    for batch in train_batches:\n",
    "        model.update(input=batch['input'], output=batch['output'])\n",
    "\n",
    "    # Prediction for this epoch\n",
    "    hat_y = model.predict(input=test_set['input'])\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = 100*np.mean(hat_y == test_set['output'])\n",
    "\n",
    "    # Inform user\n",
    "    print(\"Epoch %d: accuracy %2.2f %%\" % (epoch+1, accuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}