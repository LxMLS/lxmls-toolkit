{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon Sentiment Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ease-up the upcoming implementation exercise, examine and comment the following implementation of a log-linear model and its gradient update rule. Start by loading Amazon sentiment corpus used in day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxmls.readers.sentiment_reader as srs\n",
    "from lxmls.deep_learning.utils import AmazonData\n",
    "corpus = srs.SentimentCorpus(\"books\")\n",
    "data = AmazonData(corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.datasets['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Shallow Model: Log-Linear in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the following numpy implementation of a log-linear model with the derivations seen in the previous sections. Introduce comments on the blocks marked with # relating them to the corresponding algorithm steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.utils import Model, glorot_weight_init, index2onehot, logsumexp\n",
    "import numpy as np\n",
    "\n",
    "class NumpyLogLinear(Model):\n",
    "    \n",
    "    def __init__(self, **config):\n",
    "        \n",
    "        # Initialize parameters\n",
    "        weight_shape = (config['input_size'], config['num_classes'])\n",
    "        # after Xavier Glorot et al\n",
    "        self.weight = glorot_weight_init(weight_shape, 'softmax')\n",
    "        self.bias = np.zeros((1, config['num_classes']))\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        \n",
    "    def log_forward(self, input=None):  \n",
    "        \"\"\"Forward pass of the computation graph\"\"\"\n",
    "        \n",
    "        # Linear transformation\n",
    "        z = np.dot(input, self.weight.T) + self.bias\n",
    "        \n",
    "        # Softmax implemented in log domain\n",
    "        log_tilde_z = z - logsumexp(z, axis=1, keepdims=True)\n",
    "        \n",
    "        return log_tilde_z\n",
    "        \n",
    "    def predict(self, input=None):\n",
    "        \"\"\"Prediction: most probable class index\"\"\"\n",
    "        return np.argmax(np.exp(self.log_forward(input)), axis=1)      \n",
    "     \n",
    "    def update(self, input=None, output=None):\n",
    "        \"\"\"Stochastic Gradient Descent update\"\"\"\n",
    "        \n",
    "        # Probabilities of each class\n",
    "        class_probabilities = np.exp(self.log_forward(input))\n",
    "        batch_size, num_classes = class_probabilities.shape\n",
    "        \n",
    "        # Error derivative at softmax layer\n",
    "        I = index2onehot(output, num_classes)\n",
    "        error = (class_probabilities - I) / batch_size\n",
    "        \n",
    "        # Weight gradient\n",
    "        gradient_weight = np.zeros(self.weight.shape)\n",
    "        for l in range(batch_size):\n",
    "            gradient_weight += np.outer(error[l, :], input[l, :])\n",
    "        \n",
    "        # Bias gradient\n",
    "        gradient_bias = np.sum(error, axis=0, keepdims=True)\n",
    "        \n",
    "        # SGD update\n",
    "        self.weight = self.weight - self.learning_rate * gradient_weight\n",
    "        self.bias = self.bias - self.learning_rate * gradient_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model and data classes. Check the initial accuracy of the model. This should be close to 50% since we are on a binary prediction task and the model is not trained yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "num_epochs = 10\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NumpyLogLinear(\n",
    "    input_size=corpus.nr_features,\n",
    "    num_classes=2, \n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of epochs and batch size\n",
    "num_epochs = 10\n",
    "batch_size = 30\n",
    "\n",
    "# Get batch iterators for train and test\n",
    "train_batches = data.batches('train', batch_size=batch_size)\n",
    "test_set = data.batches('test', batch_size=None)[0]\n",
    "\n",
    "# Get intial accuracy\n",
    "hat_y = model.predict(input=test_set['input'])\n",
    "accuracy = 100*np.mean(hat_y == test_set['output'])\n",
    "print(\"Initial accuracy %2.2f %%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with simple batch stochastic gradient descent. Be sure to understand each of the steps involved, including the code running inside of the model class. We will be wokring on a more complex version of the model in the upcoming exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoch loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Batch loop\n",
    "    for batch in train_batches:\n",
    "        model.update(input=batch['input'], output=batch['output'])\n",
    "\n",
    "    # Prediction for this epoch\n",
    "    hat_y = model.predict(input=test_set['input'])\n",
    "\n",
    "    # Evaluation\n",
    "    accuracy = 100*np.mean(hat_y == test_set['output'])\n",
    "\n",
    "    # Inform user\n",
    "    print(\"Epoch %d: accuracy %2.2f %%\" % (epoch+1, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
