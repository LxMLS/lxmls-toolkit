{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eisner's algorithm step-by-step\n",
    "\n",
    "Inputs\n",
    "\n",
    "* arcs scores $s_\\theta(h,m)$, for $h\\in \\{0,...,N\\}$ and $m\\in \\{1,...,N\\}$, $h\\neq m$\n",
    "* sentence _She enjoys the Summer School._\n",
    "\n",
    "Notice that\n",
    "\n",
    "* the length of the sequence is $N = 5$\n",
    "* the terminal symbols that comprise the sentence are $w_1=$ _She_, $w_2=$ _enjoys_, $w_3=$ _the_, $w_4=$ _Summer_, $w_5=$ _School_\n",
    "* the root symbol $w_0=\\ast$ is defined for convenience; the whole sentence can be thought as being _$\\ast$ She enjoys the Summer School._\n",
    "\n",
    "Variables to fill in:\n",
    "\n",
    "* $\\mathrm{incomplete}$, shape $(N+1)\\times (N+1) \\times 2$: incomplete span scores\n",
    "* $\\mathrm{complete}$, shape $(N+1)\\times (N+1) \\times 2$: complete span scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Initialization corresponds to setting all 1-word 'spans' scores to zero. As we will see in the induction stage, these will be the initial building blocks for computing longer span scores.\n",
    "\n",
    "The figure below illustrates all the initialized span scores.\n",
    "\n",
    "![eisner_init](../../images_for_notebooks/parsing/eisner_init.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Induction\n",
    "\n",
    "We now proceed to do some iterations on the Induction stage's double `for` loop:\n",
    "\n",
    "#### Spans with $k=1$ \n",
    "\n",
    "$k=1$ corresponds to spans over pairs of words. The inner loop variable $s$ loops over the words, and determines the leftmost word in the span. The other variable $t:=s+k$ corresponds to the end of the span (the rightmost word).\n",
    "\n",
    "#### Incomplete spans\n",
    "\n",
    "Since $s\\leq r<t$, and $t=s+k=s+1$, one concludes that $r=s$ for all spans with $k=1$. For that reason, the highest score corresponds to the only value of $r$:\n",
    "\n",
    "$$\n",
    "\\mathrm{incomplete}[s,t,\\leftarrow]\\overset{(r=s)}{=}\\mathrm{complete[s,s,\\rightarrow]}+\\mathrm{complete[t,t,\\leftarrow]}+s_\\theta(t,s)\n",
    "$$\n",
    "\n",
    "![eisner_inc_left](../../images_for_notebooks/parsing/eisner_inc_left.svg)\n",
    "\n",
    "\n",
    "Notice the complete spans on the right hand side do not meet on top of a word. That is the reason why these spans are called _incomplete._\n",
    "\n",
    "The incomplete spans that go right are computed in the exact same way, except the arc score we use is the one of the arc going _right:_ $s_\\theta(s,t)$ instead of $s_\\theta(t,s)$.\n",
    "\n",
    "$$\n",
    "\\mathrm{incomplete}[s,t,\\rightarrow]\\overset{(r=s)}{=}\\mathrm{complete[s,s,\\rightarrow]}+\\mathrm{complete[t,t,\\leftarrow]}+s_\\theta(s,t)\n",
    "$$\n",
    "\n",
    "![eisner_inc_right](../../images_for_notebooks/parsing/eisner_inc_right.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spans with length $k=2$\n",
    "\n",
    "The next step is to compute scores for spans over three words. An immediate consequence is that now $r$ can take two different values: $s$ and $s+1$. Now there is an actual need to maximize the score over possible values of $r$.\n",
    "\n",
    "#### Incomplete spans\n",
    "\n",
    "The different values of $r$ correspond to using different sets of complete span scores.\n",
    "\n",
    "$$\n",
    "\\mathrm{incomplete}[s,t,\\leftarrow]=\\underset{r}{\\max}\\left\\{\\begin{matrix}(r=s)& \\mathrm{complete}[s,s,\\rightarrow]+\\mathrm{complete}[s+1,t,\\leftarrow]+s_\\theta(t,s)\\\\ (r=s+1)& \\mathrm{complete}[s,s+1,\\rightarrow]+\\mathrm{complete}[t,t,\\leftarrow]+s_\\theta(t,s)\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "![eisner_inc_left_2](../../images_for_notebooks/parsing/eisner_inc_left_2.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure to compute right-facing incomplete spans is similar to the one above. All that changes is the arc score that is used.\n",
    "\n",
    "$$\n",
    "\\mathrm{incomplete}[s,t,\\rightarrow]=\\underset{r}{\\max}\\left\\{\\begin{matrix}(r=s)& \\mathrm{complete}[s,s,\\rightarrow]+\\mathrm{complete}[s+1,t,\\leftarrow]+s_\\theta(s,t)\\\\ (r=s+1)& \\mathrm{complete}[s,s+1,\\rightarrow]+\\mathrm{complete}[t,t,\\leftarrow]+s_\\theta(s,t)\\end{matrix}\\right.\n",
    "$$\n",
    "![eisner_inc_right_2](../../images_for_notebooks/parsing/eisner_inc_right_2.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete spans\n",
    "\n",
    "We now proceed to compute complete span scores. Once again, the incomplete span scores required for this step were conveniently computed before.\n",
    "\n",
    "$$\n",
    "\\mathrm{complete}[s,t,\\leftarrow]=\\underset{r}{\\max}\\left\\{\\begin{matrix}(r=s)& \\mathrm{complete}[s,s,\\leftarrow]+\\mathrm{incomplete}[s,t,\\leftarrow]\\\\ (r=s+1)& \\mathrm{complete}[s,s+1,\\leftarrow]+\\mathrm{incomplete}[s+1,t,\\leftarrow]\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "![eisner_comp_left_2](../../images_for_notebooks/parsing/eisner_comp_left_2.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in this demo is to compute right-facing complete span scores over three words:\n",
    "\n",
    "$$\n",
    "\\mathrm{complete}[s,t,\\rightarrow]=\\underset{r}{\\max}\\left\\{\\begin{matrix}(r=s)& \\mathrm{incomplete}[s,s+1,\\rightarrow]+\\mathrm{complete}[s+1,t,\\rightarrow]\\\\ (r=s+1)& \\mathrm{incomplete}[s,t,\\rightarrow]+\\mathrm{complete}[t,t,\\rightarrow]\\end{matrix}\\right.\n",
    "$$\n",
    "![eisner_comp_right_2](../../images_for_notebooks/parsing/eisner_comp_right_2.svg)\n",
    "\n",
    "\n",
    "\n",
    "These steps continue until a complete span of size $N+1$ is computed, which corresponds to spanning the whole sentence. After that, we backtrack the highest scores to build the parse tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Eisner's algorithm\n",
    "\n",
    "Implement Eisner’s algorithm for projective dependency parsing. The pseudo-code is shown as Algorithm 13. Implement this algorithm as the function:\n",
    "\n",
    "```python\n",
    "    def parse_proj(self, scores):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in file dependency decoder.py. The input is a matrix of arc scores, whose dimension is (N + 1)-by-(N + 1), and whose (h, m) entry contains the score sq(h, m). \n",
    "\n",
    "In particular, the first row contains the scores for the arcs that depart from the root, and the first column’s values, along with the main diagonal, are to be ignored (since no arcs point to the root, and there are no self-pointing arcs). To make your job easier, we provide an implementation of the backtracking part:\n",
    "\n",
    "```python\n",
    "    def backtrack_eisner(self, incomplete_backtrack, complete_backtrack, s, t, direction, complete, heads):\n",
    "```\n",
    "    \n",
    "so you just need to build complete/incomplete spans and their backtrack pointers and then call\n",
    "\n",
    "```python\n",
    "    heads = -np.ones(N+1, dtype=int) \n",
    "    self.backtrack_eisner(incomplete_backtrack, complete_backtrack, 0, N, 1, 1,heads)\n",
    "    return heads\n",
    "```\n",
    "to obtain the final parse.\n",
    "To test the algorithm, retrain the parser on the English data (where the trees are actually all projective) by setting\n",
    "the flag dp.projective to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser() \n",
    "dp.features.use_lexical = True \n",
    "dp.features.use_distance = True \n",
    "dp.features.use_contextual = True \n",
    "dp.read_data(\"english\") \n",
    "dp.projective = True\n",
    "dp.train_perceptron(10)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following results:\n",
    "```\n",
    "    ￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼4.2.5\n",
    "    Number of sentences: 8044\n",
    "    Number of tokens: 80504\n",
    "    Number of words: 12202\n",
    "    Number of pos: 48\n",
    "    Number of features: 338014\n",
    "    Epoch 1\n",
    "    Training accuracy: 0.835637168541\n",
    "    Epoch 2\n",
    "    Training accuracy: 0.922426254687\n",
    "    Epoch 3\n",
    "    Training accuracy: 0.947621628947\n",
    "    Epoch 4\n",
    "    Training accuracy: 0.960326602521\n",
    "    Epoch 5\n",
    "    Training accuracy: 0.967689840538\n",
    "    Epoch 6\n",
    "    Training accuracy: 0.97263631025\n",
    "    Epoch 7\n",
    "    Training accuracy: 0.97619370285\n",
    "    Epoch 8\n",
    "    Training accuracy: 0.979209016579\n",
    "    Epoch 9\n",
    "    Training accuracy: 0.98127569228\n",
    "    Epoch 10\n",
    "    Training accuracy: 0.981320865519\n",
    "    Test accuracy (509 test instances): 0.886732599366\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
