{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the CRF code and the feature_mapper code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../../../')\n",
    "\n",
    "import lxmls\n",
    "import lxmls.sequences.crf_online as crfo\n",
    "import lxmls.readers.pos_corpus as pcc\n",
    "import lxmls.sequences.id_feature as idfc\n",
    "import lxmls.sequences.extended_feature as exfc\n",
    "from lxmls.readers import pos_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from the conll task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = lxmls.readers.pos_corpus.PostagCorpus()\n",
    "data_path = \"../../../../lxmls-toolkit/data/\"\n",
    "\n",
    "train_seq = corpus.read_sequence_list_conll(data_path + \"/train-02-21.conll\", \n",
    "                                            max_sent_len=10, max_nr_sent=1000)\n",
    "\n",
    "test_seq = corpus.read_sequence_list_conll(data_path + \"test-23.conll\", \n",
    "                                           max_sent_len=10, max_nr_sent=1000)\n",
    "\n",
    "dev_seq = corpus.read_sequence_list_conll(data_path + \"dev-22.conll\", \n",
    "                                          max_sent_len=10, max_nr_sent=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 examples in train_seq\n",
      "First example: Ms./noun Haag/noun plays/verb Elianti/noun ./. \n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(train_seq), \"examples in train_seq\")\n",
    "print(\"First example:\", train_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation\n",
    "\n",
    "Given a dataset, in order to build the features\n",
    "\n",
    "- An instance from **`lxmls.sequences.id_feature.IDFeatures(train_data)`** must be instanciated. We will call `feature_mapper` this instanciated object.\n",
    "- Then **`feature_mapper.build_features()`** must be executed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building features\n",
    "feature_mapper = idfc.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### About feature_mappers\n",
    "A ```feature_mapper``` will contain the following attributes:\n",
    "\n",
    "- the dataset in ```.dataset```\n",
    "    - if we instantiate the feature mapper with a dataset X then ```feature_mapper.dataset```will be a copy of X\n",
    "\n",
    "\n",
    "- a dictionary of features in ```.feature_dict```\n",
    "    - this dictionary will default to ```{}```. \n",
    "    - In order to build the features the feature mapper must call ```.build_features()``` function.\n",
    "    \n",
    "    \n",
    "- a list of features in ```.feature_list```\n",
    "    - this list will default to ```[]```. \n",
    "    - In order to build the list of features the feature mapper must call ```.build_features()``` function.\n",
    "\n",
    "A ```feature_mapper``` will contain the method \n",
    "\n",
    "- A method to generate features, ```.build_features```\n",
    "    - this method will create features using the ```.dataset``.\n",
    "    - This method will also fill ```.feature_dict``` and ```.feature_list``\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_mapper.feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0]], [[3], [5], [7], [9]], [[10]], [[1], [2], [4], [6], [8]]]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Let's see the features for the first training example\n",
    "feature_mapper.feature_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial features: [[0]]\n",
      "\n",
      "Transition features: [[3], [5], [7], [9]]\n",
      "\n",
      "Final features: [[10]]\n",
      "\n",
      "Emission features: [[1], [2], [4], [6], [8]]\n"
     ]
    }
   ],
   "source": [
    "## The previous features can be classified into:\n",
    "\n",
    "print(\"\\nInitial features:\",     feature_mapper.feature_list[0][0])\n",
    "print(\"\\nTransition features:\",  feature_mapper.feature_list[0][1])\n",
    "print(\"\\nFinal features:\",       feature_mapper.feature_list[0][2])\n",
    "print(\"\\nEmission features:\",    feature_mapper.feature_list[0][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An observation on the features for a given example\n",
    "\n",
    "All features for all the training examples in are saved in `train_seq` will be saved in ``feature_mapper.feature_list``.\n",
    "\n",
    "- If `feature_mapper.feature_list[m]` is our feature vector for training example `m`... why it's not a vector?\n",
    "\n",
    "    - Good point! In order to make the algorithm fast, the code is written using dicts, so if we access only a few positions from the dict and compute substractions it will be much faster than computing the substraction of two huge weight vectors. Notice that there are `len(feature_mapper.feature_dict)` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq), len(feature_mapper.feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codification of the features\n",
    "\n",
    "\n",
    "Features are identifyed by **init_tag:**, **prev_tag:**,  **final_prev_tag:**, **id:**\n",
    "\n",
    "- **init_tag:** when they are Initial features\n",
    "    - Example: **``init_tag:noun``** is an initial feature that describes that the first word is a noun\n",
    "    \n",
    "    \n",
    "- **prev_tag:** when they are transition features\n",
    "    - Example: **``prev_tag:noun::noun``** is an transition feature that describes that the previous word was\n",
    "      a noun and the current word is a noun.\n",
    "    - Example: **``prev_tag:noun:.``** is an transition feature that describes that the previous word was\n",
    "      a noun and the current word is a `.` (this is usually foud as the last transition feature since most phrases will end up with a dot)\n",
    "      \n",
    "\n",
    "\n",
    "- **final_prev_tag:** when they are final features\n",
    "    - Example: **``final_prev_tag:.``** is a final feature stating that the last \"word\" in the sentence was a dot.\n",
    "\n",
    "\n",
    "- **id:** when they are emission features\n",
    "    - Example: **``id:plays::verb``** is an emission feature, describing that the current word is plays and the current hidden state is a verb.\n",
    "    - Example: **``id:Feb.::noun``** is an emission feature, describing that the current word is \"Feb.\" and the current hidden state is a noun.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_feature_dict = {word: pos for pos, word in feature_mapper.feature_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_mapper.feature_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['init_tag:noun']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prev_tag:noun::noun',\n",
       " 'prev_tag:noun::verb',\n",
       " 'prev_tag:verb::noun',\n",
       " 'prev_tag:noun::.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_prev_tag:.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[inv_feature_dict[x[0]] for x in feature_mapper.feature_list[0][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27198"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_seq.x_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Objective value: -5.779018\n",
      "Epoch: 1 Objective value: -3.192724\n",
      "Epoch: 2 Objective value: -2.717537\n",
      "Epoch: 3 Objective value: -2.436614\n",
      "Epoch: 4 Objective value: -2.240491\n",
      "Epoch: 5 Objective value: -2.091833\n",
      "Epoch: 6 Objective value: -1.973353\n",
      "Epoch: 7 Objective value: -1.875643\n",
      "Epoch: 8 Objective value: -1.793034\n",
      "Epoch: 9 Objective value: -1.721857\n",
      "Epoch: 10 Objective value: -1.659605\n",
      "Epoch: 11 Objective value: -1.604499\n",
      "Epoch: 12 Objective value: -1.555229\n",
      "Epoch: 13 Objective value: -1.510806\n",
      "Epoch: 14 Objective value: -1.470468\n",
      "Epoch: 15 Objective value: -1.433612\n",
      "Epoch: 16 Objective value: -1.399759\n",
      "Epoch: 17 Objective value: -1.368518\n",
      "Epoch: 18 Objective value: -1.339566\n",
      "Epoch: 19 Objective value: -1.312636\n"
     ]
    }
   ],
   "source": [
    "## Train crf\n",
    "crf_online = crfo.CRFOnline(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "crf_online.num_epochs = 20\n",
    "crf_online.train_supervised(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## You will receive feedback when each epoch is finished,\n",
    "## note that running the 20 epochs might take a while. After training is done,\n",
    "## evaluate the learned model on the training, development and test sets.\n",
    "\n",
    "pred_train = crf_online.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = crf_online.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = crf_online.viterbi_decode_corpus(test_seq)\n",
    "\n",
    "eval_train = crf_online.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = crf_online.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = crf_online.evaluate_corpus(test_seq, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF - ID Features Accuracy Train: 0.949 Dev: 0.846 Test: 0.858\n"
     ]
    }
   ],
   "source": [
    "print(\"CRF - ID Features Accuracy Train: %.3f Dev: %.3f Test: %.3f\" \\\n",
    "       %(eval_train,eval_dev, eval_test))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
