{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.10\n",
    "Over the next couple of exercises we will make use of the Galton dataset, a dataset of heights of fathers\n",
    "and sons from the 1877 paper that first discussed the “regression to the mean” phenomenon. This dataset has 928 pairs\n",
    "of numbers.\n",
    "* Use the ``load()`` function in the ``galton.py`` file to load the dataset. The file is located under the lxmls/readers folder. Type the following in your Python interpreter:\n",
    "\n",
    "``import galton as galton\n",
    "galton_data = galton.load()``\n",
    "\n",
    "* What are the mean height and standard deviation of all the people in the sample? What is the mean height of the\n",
    "fathers and of the sons?\n",
    "* Plot a histogram of all the heights (you might want to use the plt.hist function and the ravel method on\n",
    "arrays).\n",
    "* Plot the height of the father versus the height of the son.\n",
    "* You should notice that there are several points that are exactly the same (e.g., there are 21 pairs with the values 68.5\n",
    "and 70.2). Use the ``?`` command in ipython to read the documentation for the numpy.random.randn function\n",
    "and add random jitter (i.e., move the point a little bit) to the points before displaying them. Does your impression\n",
    "of the data change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.readers import galton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton_data = galton.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(galton_data.mean(0))\n",
    "print(galton_data.std(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(galton_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(galton_data[:,0], galton_data[:,1], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.randn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton_data_randn = galton_data + 0.5*np.random.randn(len(galton_data), 2)\n",
    "plt.plot(galton_data_randn[:,0], galton_data_randn[:,1], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.11 \n",
    "Consider the function $f(x) = x^2$ and its derivative \n",
    "$ \\frac{\\partial f}{\\partial x} $.\n",
    "Look at the derivative of that function at points ``[-2,0,2]``, draw the tangent to the graph in that point \n",
    "$ \\frac{\\partial f}{\\partial x}(-2)=-4 $, \n",
    "$ \\frac{\\partial f}{\\partial x}(0)=0 $ and \n",
    "$ \\frac{\\partial f}{\\partial x}(2)=4 $. \n",
    "\n",
    "For example, the tangent\n",
    "equation for $x = −2$ is $y = −4x − b$, where $b = f(−2)$. The following code plots the function and the derivatives on\n",
    "those points using matplotlib (See Figure 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = np.arange(-5,5,0.01)\n",
    "f_x = np.power(a,2)\n",
    "plt.plot(a,f_x)\n",
    "plt.xlim(-5,5)\n",
    "plt.ylim(-5,15)\n",
    "k = np.array([-2,0,2])\n",
    "plt.plot(k,k**2,\"bo\")\n",
    "for i in k:\n",
    "    plt.plot(a, (2*i)*a - (i**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.12 \n",
    "Consider the function $f(x) = (x + 2)^2 − 16 \\text{exp}(−(x − 2)^2)$\n",
    ". Make a function that computes the\n",
    "function value given x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(x):\n",
    "    return (x+2)**2 - 16*np.exp(-((x-2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a plot around $x \\in [−8, 8]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-8,8,0.001)\n",
    "y = list(map(lambda u: get_y(u),x))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the derivative of the function $f(x)$, implement the function ``get grad(x)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(x):\n",
    "    return (2*x+4)-16*(-2*x + 4)*np.exp(-((x-2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method gradient descent to find the minimum of this function. Convince yourself that the code is doing the\n",
    "proper thing. Look at the constants we defined. Note, that we are using a simple approach to pick the step size (always\n",
    "have the value step size) which is not necessarily correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(start_x,func,grad, step_size=0.1):\n",
    "    # Precision of the solution\n",
    "    prec = 0.0001\n",
    "    #Use a fixed small step size\n",
    "    #step_size = 0.1\n",
    "    #max iterations\n",
    "    max_iter = 100\n",
    "    x_new = start_x\n",
    "    res = []\n",
    "    for i in range(max_iter):\n",
    "        x_old = x_new\n",
    "        #Use beta egual to -1 for gradient descent\n",
    "        x_new = x_old - step_size * grad(x_new)\n",
    "        f_x_new = func(x_new)\n",
    "        f_x_old = func(x_old)\n",
    "        res.append([x_new,f_x_new])\n",
    "        if(abs(f_x_new - f_x_old) < prec):\n",
    "            print(\"change in function values too small, leaving\")\n",
    "            return np.array(res)\n",
    "    print(\"exceeded maximum number of iterations, leaving\")\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the gradient descent algorithm starting from $x_0 = −8$ and plot the minimizing sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = -8\n",
    "res = gradient_descent(x_0,get_y,get_grad)\n",
    "plt.plot(res[:,0],res[:,1],'+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = 8\n",
    "res = gradient_descent(x_0,get_y,get_grad, step_size=0.01)\n",
    "plt.plot(res[:,0],res[:,1],'+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.13 \n",
    "1) Consider the function $f(x) = (x + 2)^2 − 16 \\text{exp}(−(x − 2)^2)$. Draw a plot around the $x \\in [−8, 8]$ region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-8,8,0.001)\n",
    "y = list(map(lambda u: get_y(u),x))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)  What is $\\frac{\\partial f}{\\partial x}$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Use gradient descent to find a local minimum starting from $x0 = −4$ and $x0 = +4$, with $η = .01$. Plot all of the intermediate estimates that you obtain in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(-8, 8, 1000)\n",
    "Y = (X+2)**2 - 16*np.exp(-((X-2)**2))\n",
    "\n",
    "# derivative of the function f\n",
    "def get_Y_dev(x):\n",
    "    return (2*x+4)-16*(-2*x + 4)*np.exp(-((x-2)**2))\n",
    "\n",
    "def grad_desc(start_x, eps, prec):\n",
    "    '''\n",
    "    runs the gradient descent algorithm and returns the list of estimates\n",
    "    example of use grad_desc(start_x=3.9, eps=0.01, prec=0.00001)\n",
    "    '''\n",
    "    x_new = start_x\n",
    "    x_old = start_x + prec * 2\n",
    "    res = [x_new]\n",
    "    while abs(x_old-x_new) > prec:\n",
    "        x_old = x_new\n",
    "        x_new = x_old - eps * get_Y_dev(x_new)\n",
    "        res.append(x_new)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = grad_desc(4, 0.01, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res, (res+2)**2 - 16*np.exp(-((res-2)**2)), '+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.14 \n",
    "Consider the linear regression problem (ordinary least squares) on the Galton dataset, with a single response\n",
    "variable\n",
    "\n",
    "\\begin{equation*}\n",
    "y = x^Tw + ε\n",
    "\\end{equation*}\n",
    "\n",
    "The linear regression problem is, given a set $\\{y\n",
    "(i)\\}_i$ of samples of $y$ and the corresponding $x^{(i)}$ vectors, estimate w\n",
    "to minimise the sum of the $\\epsilon$ variables. Traditionally this is solved analytically to obtain a closed form solution (although\n",
    "this is **not the way in which it should be computed** in this exercise, linear algebra packages have an optimised solver,\n",
    "with numpy, use ``numpy.linalg.lstsq``).\n",
    "Alternatively, we can define the error function for each possible $w$:\n",
    "\n",
    "\\begin{equation*}\n",
    "e(w) = \\sum_i ( x^{(i)^T} w - y^{(i)} )^2.\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Derive the gradient of the erro $\\frac{\\partial e}{\\partial w_j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R:\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial e}{\\partial w_j} = \\sum_i 2 x_j^{(i)} ( x^{(i)^T} w - y^{(i)} ).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implement a solver based on this for two dimensional problem (*i.e.*, $w \\in R^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Use this method on the Galton dataset from the previous exercise to estimate the relationship between father and son’s height. Try two formulas\n",
    "\n",
    "$s = f w_1 + \\epsilon$, \n",
    "\n",
    "where s is the son’s height, and f is the father heights; and\n",
    "\n",
    "$s = f w_1 + 1w_0 + \\epsilon$,\n",
    "\n",
    "where the input variable is now two dimensional: $(f , 1)$. This allows the intercept to be non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data.\n",
    "use_bias = True #True\n",
    "y = galton_data[:,0] \n",
    "if use_bias:\n",
    "    x = np.vstack( [galton_data[:,1], np.ones(galton_data.shape[0])] )  \n",
    "else:\n",
    "    x = np.vstack( [galton_data[:,1], np.zeros(galton_data.shape[0])] )\n",
    "    \n",
    "# derivative of the error function e\n",
    "def get_e_dev(w, x,y): # y, x, \n",
    "    error_i = np.matmul(w, x) - y\n",
    "    derro_dw = np.matmul(2*x, error_i)/len(y)\n",
    "    # print(derro_dw, np.multiply(error_i,error_i).sum())\n",
    "    return derro_dw\n",
    "\n",
    "# Initialize w.\n",
    "w = np.array([1,0])\n",
    "\n",
    "#get_e_dev(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize w.\n",
    "w = np.array([0.5,50.0])\n",
    "\n",
    "def grad_desc(start_x, eps, prec, x,y): #gradient=get_e_dev\n",
    "    '''\n",
    "    runs the gradient descent algorithm and returns the list of estimates\n",
    "    example of use grad_desc(start_x=3.9, eps=0.01, prec=0.00001)\n",
    "    '''\n",
    "    x_new = start_x\n",
    "    x_old = start_x + prec * 2\n",
    "    res = [x_new]\n",
    "    mses = []\n",
    "    while abs(x_old-x_new).sum() > prec:\n",
    "        x_old = x_new\n",
    "        x_new = x_old - eps * get_e_dev(x_new, x,y)\n",
    "        res.append(x_new)\n",
    "        \n",
    "        error_i = np.matmul(x_new, x) - y\n",
    "        mse = np.multiply(error_i, error_i).sum()\n",
    "        mses.append(mse)\n",
    "        print(x_new, mse)\n",
    "    return np.array(res), np.array(mses)\n",
    "\n",
    "res, mses = grad_desc(w, 0.0002, 0.00001, x,y)\n",
    "w_sgd = res[-1]\n",
    "w_sgd\n",
    "#res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Plot the regression line you obtain with the points from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res[:, 0], res[:, 1], '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(y - np.matmul(w,x), y - np.matmul(w,x)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Use the ``np.linalg.lstsq`` function and compare to your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import lstsq\n",
    "m, c = np.linalg.lstsq(x.T, y)[0]\n",
    "print(m,c)\n",
    "w_lstsq = np.array([m, c])\n",
    "error2  = np.multiply(y - np.matmul(w_lstsq,x), y - np.matmul(w_lstsq,x)).sum()\n",
    "print(error2)\n",
    "#lstsq(a=, b=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting regressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(galton_data[:,0],galton_data[:,1], \".\")\n",
    "plt.title(\"We nailed it??\")\n",
    "maxim, minim = int(np.max(galton_data[:,0])), int(np.min(galton_data[:,0]))\n",
    "xvals = [vec for vec in np.array(range(minim-1, maxim+1)) ]\n",
    "\n",
    "# Gradient descent solution\n",
    "yvals = [ w_sgd[0]  * xval + w_sgd[1] for xval in xvals]\n",
    "\n",
    "# solution from closed form\n",
    "yvals2 = [ w_lstsq[0]  * xval + w_lstsq[1] for xval in xvals]\n",
    "\n",
    "plt.plot(xvals, yvals, '--', c='k',linewidth=2)\n",
    "plt.plot(xvals, yvals2, '--', c='r',linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.15 \n",
    "Use the debugger to debug the ``buggy.py`` script which attempts to repeatedly perform the following\n",
    "computation:...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start $x_0 = 0$\n",
    "2. Iterate\n",
    "\n",
    "    (a) $x'_{t+1} = x_t + r$, where $r$ is a random variable.\n",
    "\n",
    "    (b) if $x'_{t+1} >= 1$, then stop.\n",
    "    \n",
    "    (c) if $x'_{t+1} <= 0$, then $x_{t+1} = 0$\n",
    "\n",
    "    (d) else $x_{t+1} = x'_{t+1}$\n",
    "    \n",
    "\n",
    "3. Return the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having repeated this computation a number of times, the programme prints the average. Unfortunately, the program\n",
    "has a few bugs, which you need to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_x(x):\n",
    "    x += np.random.normal(scale=.0625)\n",
    "    if x < 0:\n",
    "        return 0.\n",
    "    return x\n",
    "\n",
    "\n",
    "def walk():\n",
    "    iters = 0\n",
    "    x = 0\n",
    "    while x <= 1.:\n",
    "        x = next_x(x)\n",
    "        iters += 1\n",
    "    return iters\n",
    "\n",
    "\n",
    "walks = np.array([walk() for i in range(1000)])\n",
    "print(np.mean(walks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
