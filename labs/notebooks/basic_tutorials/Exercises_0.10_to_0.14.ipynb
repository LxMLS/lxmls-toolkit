{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.10\n",
    "Over the next couple of exercises we will make use of the Galton dataset, a dataset of heights of fathers\n",
    "and sons from the 1877 paper that first discussed the “regression to the mean” phenomenon. This dataset has 928 pairs\n",
    "of numbers.\n",
    "* Use the ``load()`` function in the ``galton.py`` file to load the dataset. The file is located under the lxmls/readers folder. Type the following in your Python interpreter:\n",
    "\n",
    "``import galton as galton\n",
    "galton_data = galton.load()``\n",
    "\n",
    "* What are the mean height and standard deviation of all the people in the sample? What is the mean height of the\n",
    "fathers and of the sons?\n",
    "* Plot a histogram of all the heights (you might want to use the plt.hist function and the ravel method on\n",
    "arrays).\n",
    "* Plot the height of the father versus the height of the son.\n",
    "* You should notice that there are several points that are exactly the same (e.g., there are 21 pairs with the values 68.5\n",
    "and 70.2). Use the ``?`` command in ipython to read the documentation for the numpy.random.randn function\n",
    "and add random jitter (i.e., move the point a little bit) to the points before displaying them. Does your impression\n",
    "of the data change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.readers import galton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton_data = galton.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(galton_data.mean(0))\n",
    "print(galton_data.std(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(galton_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(galton_data[:,0], galton_data[:,1], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.randn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galton_data_randn = galton_data + 0.5*np.random.randn(len(galton_data), 2)\n",
    "plt.plot(galton_data_randn[:,0], galton_data_randn[:,1], '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.11 \n",
    "Consider the function $f(x) = x^2$ and its derivative \n",
    "$ \\frac{\\partial f}{\\partial x} $.\n",
    "Look at the derivative of that function at points ``[-2,0,2]``, draw the tangent to the graph in that point \n",
    "$ \\frac{\\partial f}{\\partial x}(-2)=-4 $, \n",
    "$ \\frac{\\partial f}{\\partial x}(0)=0 $ and \n",
    "$ \\frac{\\partial f}{\\partial x}(2)=4 $. \n",
    "\n",
    "For example, the tangent\n",
    "equation for $x = -2$ is $y = -4x - b$, where $b = f(-2)$. The following code plots the function and the derivatives on\n",
    "those points using matplotlib (See Figure 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = np.arange(-5,5,0.01)\n",
    "f_x = np.power(a,2)\n",
    "plt.plot(a,f_x)\n",
    "plt.xlim(-5,5)\n",
    "plt.ylim(-5,15)\n",
    "k = np.array([-2,0,2])\n",
    "plt.plot(k,k**2,\"bo\")\n",
    "for i in k:\n",
    "    plt.plot(a, (2*i)*a - (i**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.12 \n",
    "Consider the function $f(x) = (x + 2)^2 - 16 \\text{exp}(-(x - 2)^2)$\n",
    ". Make a function that computes the\n",
    "function value given x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y(x):\n",
    "    return (x+2)**2 - 16*np.exp(-((x-2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a plot around $x \\in [-8, 8]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-8, 8, 0.001)\n",
    "y = get_y(x)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the derivative of the function $f(x)$, implement the function ``get grad(x)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(x):\n",
    "    return (2*x+4)-16*(-2*x + 4)*np.exp(-((x-2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method gradient descent to find the minimum of this function. Convince yourself that the code is doing the\n",
    "proper thing. Look at the constants we defined. Note, that we are using a simple approach to pick the step size (always\n",
    "have the value step size) which is not necessarily correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_scalar(start_x, func, grad, step_size=0.1, prec=0.0001):\n",
    "    max_iter=100\n",
    "    x_new = start_x\n",
    "    res = []\n",
    "    for i in range(max_iter):\n",
    "        x_old = x_new\n",
    "        #Use negative step size for gradient descent\n",
    "        x_new = x_old - step_size * grad(x_new)\n",
    "        f_x_new = func(x_new)\n",
    "        f_x_old = func(x_old)\n",
    "        res.append([x_new,f_x_new])\n",
    "        \n",
    "        if(abs(f_x_new - f_x_old) < prec):\n",
    "            print(\"change in function values too small, leaving\")\n",
    "            return np.array(res)\n",
    "    print(\"exceeded maximum number of iterations, leaving\")\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the gradient descent algorithm starting from $x_0 = -8$ and plot the minimizing sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-8, 8, 0.001)\n",
    "y = get_y(x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "x_0 = -8\n",
    "res = gradient_descent_scalar(x_0, get_y, get_grad)\n",
    "plt.plot(res[:,0], res[:,1], 'r+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the algorithm converged to a minimum, but since the function is not convex it converged only to a local minimum.\n",
    "Now try the same exercise starting from the initial point $x_0 = 8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-8, 8, 0.001)\n",
    "y = get_y(x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "x_0 = 8\n",
    "res = gradient_descent_scalar(x_0, get_y, get_grad)\n",
    "plt.plot(res[:,0], res[:,1], 'r+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now the algorithm converged to the global minimum. However, note that to get to the global minimum the sequence of points jumped from one side of the minimum to the other. This is a consequence of using a wrong step size (in this case too large). Repeat the previous exercise changing both the values of the step-size and the precision. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-8, 8, 0.001)\n",
    "y = get_y(x)\n",
    "plt.plot(x, y)\n",
    "\n",
    "x_0 = 8\n",
    "res = gradient_descent_scalar(x_0, get_y, get_grad, step_size=0.01)\n",
    "plt.plot(res[:,0], res[:,1], 'r+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.13 \n",
    "Consider the linear regression problem (ordinary least squares) on the Galton dataset, with a single response\n",
    "variable\n",
    "\n",
    "\\begin{equation*}\n",
    "y = x^Tw + ε\n",
    "\\end{equation*}\n",
    "\n",
    "The linear regression problem is, given a set $\\{y\n",
    "(i)\\}_i$ of samples of $y$ and the corresponding $x^{(i)}$ vectors, estimate w\n",
    "to minimise the sum of the $\\epsilon$ variables. Traditionally this is solved analytically to obtain a closed form solution (although\n",
    "this is **not the way in which it should be computed** in this exercise, linear algebra packages have an optimised solver,\n",
    "with numpy, use ``numpy.linalg.lstsq``).\n",
    "Alternatively, we can define the error function for each possible $w$:\n",
    "\n",
    "\\begin{equation*}\n",
    "e(w) = \\sum_i ( x^{(i)^T} w - y^{(i)} )^2.\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Derive the gradient of the error $\\frac{\\partial e}{\\partial w_j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{\\partial e}{\\partial w_j} = \\sum_i 2 x_j^{(i)} ( x^{(i)^T} w - y^{(i)} ).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Implement a solver based on this for two dimensional problem (*i.e.*, $w \\in R^2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Use this method on the Galton dataset from the previous exercise to estimate the relationship between father and son’s height. Try two formulas\n",
    "\n",
    "$s = f w_1 + \\epsilon$, \n",
    "\n",
    "where s is the son’s height, and f is the father heights; and\n",
    "\n",
    "$s = f w_1 + 1w_0 + \\epsilon$,\n",
    "\n",
    "where the input variable is now two dimensional: $(f , 1)$. This allows the intercept to be non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data.\n",
    "use_bias = True #True\n",
    "y = galton_data[:,0] \n",
    "if use_bias:\n",
    "    x = np.vstack( [galton_data[:,1], np.ones(galton_data.shape[0])] )  \n",
    "else:\n",
    "    x = np.vstack( [galton_data[:,1], np.zeros(galton_data.shape[0])] )\n",
    "    \n",
    "# derivative of the error function e\n",
    "def get_e_dev(w, x, y): # y, x, \n",
    "    error_i = np.matmul(w, x) - y\n",
    "    derro_dw = np.matmul(2*x, error_i) / len(y)\n",
    "    # print(derro_dw, np.multiply(error_i,error_i).sum())\n",
    "    return derro_dw\n",
    "\n",
    "# Initialize w.\n",
    "w = np.array([1,0])\n",
    "\n",
    "#get_e_dev(w, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize w.\n",
    "w = np.array([0.5,50.0])\n",
    "\n",
    "def gradient_descent_linear_regression(start_w, step_size, prec, x,y): #gradient=get_e_dev\n",
    "    '''\n",
    "    runs the gradient descent algorithm and returns the list of estimates\n",
    "    '''\n",
    "    w_new = start_w\n",
    "    w_old = start_w + prec * 2\n",
    "    res = [w_new]\n",
    "    mses = []\n",
    "    while abs(w_old-w_new).sum() > prec:\n",
    "        w_old = w_new\n",
    "        w_new = w_old - step_size * get_e_dev(w_new, x, y)\n",
    "        res.append(w_new)\n",
    "        \n",
    "        error_i = np.matmul(w_new, x) - y\n",
    "        mse = np.multiply(error_i, error_i).mean()\n",
    "        mses.append(mse)\n",
    "        print(w_new, mse)\n",
    "    return np.array(res), np.array(mses)\n",
    "\n",
    "res, mses = gradient_descent_linear_regression(w, 0.0002, 0.00001, x, y)\n",
    "w_sgd = res[-1]\n",
    "w_sgd\n",
    "#res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Plot the regression line you obtain with the points from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(res[:, 0], res[:, 1], '*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.multiply(y - np.matmul(w_sgd,x), y - np.matmul(w_sgd,x)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Use the ``np.linalg.lstsq`` function and compare to your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import lstsq\n",
    "m, c = np.linalg.lstsq(x.T, y)[0]\n",
    "print(m,c)\n",
    "w_lstsq = np.array([m, c])\n",
    "error2  = np.multiply(y - np.matmul(w_lstsq,x), y - np.matmul(w_lstsq,x)).sum()\n",
    "print(error2)\n",
    "#lstsq(a=, b=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting regressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(galton_data_randn[:,0], galton_data_randn[:,1], \".\")\n",
    "plt.title(\"We nailed it??\")\n",
    "maxim, minim = int(np.max(galton_data_randn[:,0])), int(np.min(galton_data_randn[:,0]))\n",
    "xvals = np.array(range(minim-1, maxim+1))\n",
    "\n",
    "# Gradient descent solution\n",
    "yvals = w_sgd[0] * xvals + w_sgd[1]\n",
    "plt.plot(xvals, yvals, '--', c='k',linewidth=2)\n",
    "\n",
    "# solution from closed form\n",
    "yvals2 = w_lstsq[0]  * xvals + w_lstsq[1]\n",
    "plt.plot(xvals, yvals2, '--', c='r',linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0.14 \n",
    "Use the debugger to debug the ``buggy.py`` script which attempts to repeatedly perform the following\n",
    "computation:...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start $x_0 = 0$\n",
    "2. Iterate\n",
    "\n",
    "    (a) $x'_{t+1} = x_t + r$, where $r$ is a random variable.\n",
    "\n",
    "    (b) if $x'_{t+1} >= 1$, then stop.\n",
    "    \n",
    "    (c) if $x'_{t+1} <= 0$, then $x_{t+1} = 0$\n",
    "\n",
    "    (d) else $x_{t+1} = x'_{t+1}$\n",
    "\n",
    "3. Return the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having repeated this computation a number of times, the programme prints the average. Unfortunately, the program\n",
    "has a few bugs, which you need to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_x(x):\n",
    "    x += np.random.normal(scale=.0625)\n",
    "    if x < 0:\n",
    "        return 0.\n",
    "    return xnext\n",
    "\n",
    "\n",
    "def walk():\n",
    "    iters = 0\n",
    "    while x <= 1.:\n",
    "        x = next_x(x)\n",
    "        iters += 1\n",
    "    return nr_iters\n",
    "\n",
    "\n",
    "walks = np.array([walk() for i in range(1000)])\n",
    "\n",
    "print(np.mean(walks))"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}