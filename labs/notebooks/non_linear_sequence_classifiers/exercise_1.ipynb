{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Sequence Models in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.1 \n",
    "Convince yourself that a RNN is just an FF unfolded in time. Complete the `backpropagation()` method in the `NumpyRNN` class in \n",
    "\n",
    "    lxmls/deep_learning/numpy_models/rnn.py \n",
    "    \n",
    "and compare it with\n",
    "\n",
    "    lxmls/deep_learning/numpy_models.mlp.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WSJ Data\n",
    "To work with RNNs we will use the Part-of-speech data-set seen in the sequence models day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Part-of-Speech data \n",
    "from lxmls.readers.pos_corpus import PostagCorpusData\n",
    "data = PostagCorpusData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and configure the NumpyRNN. Remember to use reload if you want to modify the code inside the rnns module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.numpy_models.rnn import NumpyRNN\n",
    "model = NumpyRNN(\n",
    "    input_size=data.input_size,\n",
    "    embedding_size=50,\n",
    "    hidden_size=20,\n",
    "    output_size=data.output_size,\n",
    "    learning_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestone 1:\n",
    "\n",
    "As in the case of the feed-forward networks you can use the following setup to test step by step the implementation of the gradients. First compute the cost variation for the variation of a single weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x.shape for x in model.parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxmls.deep_learning.rnn import get_rnn_parameter_handlers, get_rnn_loss_range\n",
    "\n",
    "# Get functions to get and set values of a particular weight of the model\n",
    "get_parameter, set_parameter = get_rnn_parameter_handlers(\n",
    "    layer_index=-1,\n",
    "    row=0, \n",
    "    column=0\n",
    ")\n",
    "\n",
    "# Get batch of data\n",
    "batch = data.batches('train', batch_size=1)[0]\n",
    "\n",
    "# Get loss and weight value\n",
    "current_loss = model.cross_entropy_loss(batch['input'], batch['output'])\n",
    "current_weight = get_parameter(model.parameters)\n",
    "\n",
    "# Get range of values of the weight and loss around current parameters values\n",
    "weight_range, loss_range = get_rnn_loss_range(model, get_parameter, set_parameter, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then conmpute the desired gradient from your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gradient value for that weight\n",
    "gradients = model.backpropagation(batch['input'], batch['output'])\n",
    "current_gradient = get_parameter(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally call matlplotlib to plot the loss variation versus the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot empirical\n",
    "plt.plot(weight_range, loss_range)\n",
    "plt.plot(current_weight, current_loss, 'xr')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('weight value')\n",
    "# Plot real\n",
    "h = plt.plot(\n",
    "    weight_range,\n",
    "    current_gradient*(weight_range - current_weight) + current_loss, \n",
    "    'r--'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Milestone 2:\n",
    "After you have completed the gradients you can run the model in the POS task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 20\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get batch iterators for train and test\n",
    "train_batches = data.batches('train', batch_size=1)\n",
    "dev_set = data.batches('dev', batch_size=1)\n",
    "test_set = data.batches('test', batch_size=1)\n",
    "\n",
    "# Epoch loop\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Batch loop\n",
    "    for batch in train_batches:\n",
    "        model.update(input=batch['input'], output=batch['output'])\n",
    "\n",
    "    # Evaluation dev\n",
    "    is_hit = []\n",
    "    for batch in dev_set:\n",
    "        is_hit.extend(model.predict(input=batch['input']) == batch['output'])\n",
    "    accuracy = 100*np.mean(is_hit)\n",
    "    print(\"Epoch %d: dev accuracy %2.2f %%\" % (epoch+1, accuracy))\n",
    "\n",
    "print(\"Training took %2.2f seconds per epoch\" % ((time.time() - start)/num_epochs))    \n",
    "    \n",
    "# Evaluation test\n",
    "is_hit = []\n",
    "for batch in test_set:\n",
    "    is_hit.extend(model.predict(input=batch['input']) == batch['output'])\n",
    "accuracy = 100*np.mean(is_hit)\n",
    "\n",
    "# Inform user\n",
    "print(\"Test accuracy %2.2f %%\" % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
